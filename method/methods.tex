\section{Materials and methods}

\subsection{Functional Annotation of Maize Genes}
Three sequence-based approaches were used to annotate function to genes in the maize reference genome: sequence similarity, protein domain, and mixed-method pipelines (See Figure \ref{fig:methods}; also described in sections \ref{meth:seq_sim}, \ref{meth:prot_dom} and \ref{meth:mix_meth}, respectively). The scripts (bash, R and Python) used to generate the annotations for maize B73 RefGen\_v3 are available at  \href{https://github.com/Dill-PICL/maize-GAMER}{https://github.com/Dill-PICL/maize-GAMER}. These scripts run free and open-source tools on different inputs required for these tools to generate annotation datasets. Please refer to the reproducibility supplemental file for details on versions of software, version of input datasets used and commands and parameters used to run these tools.

The B73 genome and protein sequences for gene models included in the Filtered Gene Set (FGS) were downloaded from Gramene Release 42 (\cite{telloruiz_2016-qu}). The downloaded protein FASTA file contained sequences for all FGS transcripts (e.g., the gene model X has transcript models X\_T01, X\_T02, and X\_T03). For each gene model only the longest translated protein sequence derived from the transcripts was analyzed. The gold standard annotations used for evaluations were obtained from MaizeGDB, and they encompass GO annotations for \num{1619} gene models from RefGen\_v3. The number of annotations for cellular component, molecular function, and biological process were \num{1584}, \num{88}, \num{323} respectively.

\subsubsection{Sequence similarity-based annotation} \label{meth:seq_sim}
The sequence similarity based annotation method has three main steps: 1) sequence similarity calculation, 2) valid hit detection, and 3) inheritance of high-confidence GO annotations. BLASTP was used (\cite{altschul_1990-O4}) with default parameters to calculate sequence similarity between maize protein sequences and two other datasets: the "Arabidopsis" dataset from The Arabidopsis Information Resource (TAIR) (\cite{berardini_2015-Nk}) and the “Plant” dataset from UniProt (\cite{uniprotconsortium_2015-Ky}). Valid hits were detected using the reciprocal-best-hit (RBH) method from BLASTP results. GO terms with non-reviewed ECs (i.e., IEA, NAS, and ND - described in the introduction) were removed from input datasets. All others were inherited between the RBH pairs of maize and the other plant.

%\paragraph{Maize GO annotation using \emph{Arabidopsis} dataset\\}
Arabidopsis has the largest number of reviewed (human curated) EC GO annotations among plant model organisms (see Supp Table \ref{tbl:uniprot_plants}). A FASTA file of arabidopsis protein sequences along with the cognate GO Annotation File (GAF) were downloaded from TAIR (v.10) (\cite{berardini_2015-Nk}). The TAIR protein file contained predicted protein sequences from all transcripts. This file was filtered to retain only the protein sequence derived from longest transcript for each gene. Retained protein sequences from TAIR were used to create the TAIR BLAST database, and maize protein sequences were used to create a maize BLAST database. Maize protein sequences were used to query the TAIR BLAST database. Likewise, TAIR sequences were used to query the maize BLAST database.  Results from both searches were used to detect RBH pairs between arabidopsis and maize. All non-reviewed EC GO annotations were removed, and remaining GO associations to arabidopsis genes were inherited to maize genes for each RBH pair. This maize/arabidopsis RBH ortholog dataset is called "maize-TAIR GO annotations."


All reviewed EC GO annotations and protein sequences for all plants from the UniProt-GOA database were downloaded using the QuickGO tool hosted at EBI. Protein sequences and reviewed EC GO annotations were downloaded separately. The UniProt plant GO annotation dataset containing \num{304426} annotations from \num{75537} unique protein sequences. The protein sequences downloaded spanned 292 taxa. Only ten species had more than \num{1000} annotations (see Supplementary Table \ref{tbl:uniprot_plants}). Annotations from the top 10 species (in terms of number of reviewed GO annotations) were retained for our analyses.

The process to annotate maize genes using UniProt plant data was similar to that for arabidopsis. Maize protein sequences were matched against protein sequences from each species separately using BLASTP. Putative orthologs were determined using RBH for each maize-plant pair. Terms annotated to the other plant protein were inherited to the maize protein sequence for each putative ortholog pair. GO annotations inherited from each plant species were concatenated together. The derived dataset is called the "maize-UniProt GO annotations."

\subsubsection {Protein Domains} \label{meth:prot_dom}
InterProScan5 (IPRS) version 5.16-55.0 was used to create domain based GO annotation of maize protein coding genes. IPRS was used to annotate GO terms to maize genes to produce the "maize-IPRS GO annotations."

\subsubsection{Mixed-Method Pipelines} \label{meth:mix_meth}
At the beginning of this project, the first iteration of the CAFA challenge (CAFA1; described in the Introduction) had been completed. The results from the challenge indicated that CAFA1 mixed-method pipelines performed as well or better than standard methods (\cite{radivojac_2013-YN}). To determine their predictive power for functional annotation in plants, the top-performing mixed-method pipelines from CAFA1 were reviewed to identify a group that could be implemented based upon availability of code and sufficient documentation. Three tools were selected: Argot2, FANN-GO, and PANNZER (\cite{falda_2012-VX,clark_2011--Z,koskinen_2015-sl}.)

\paragraph{Argot2\\}
Argot2 has a batch processing tool that can annotate up to \num{5000} pre-processed input sequences. There are two different pre-processing steps for Argot2: 1) querying the UniProt database for sequence similarity matches to the input sequences, and 2) querying the the Pfam database for putative domains present in the input sequences. The maize sequences were split into multiple FASTA files containing a maximum of \num{5000} sequences. The eight FASTA files resulting from the previous step were used to query the UniProt database using BLASTP for matches and the output was saved. HMMER was used to search a local Pfam database for potential hits for all the input protein sequences. Pre-processing each input FASTA resulted in a pair of input files for Argot2: BLAST and HMMER files. Each pair of  pre-processed files was compressed and submitted to Argot2 batch processing tool. Results from each pair of pre-processed data were downloaded and concatenated to create the "maize-Argot2 GO annotations."

\paragraph{FANN-GO\\}
The file containing maize protein sequences was imported into MATLAB using a built-in function. The MAIN function from FANN-GO was used to pre-process and annotate maize protein sequences. FANN-GO uses BLASTP to query FANN-GO training sequence dataset (derived from UniProt) for potential matches for the input sequences and converts the results to input feature vectors. The FANN-GO predictor built from the training dataset is then used to process the input feature vectors and calculate the probability that a particular protein is associated to a particular GO term. These probabilities are represented in a matrix where rows represent sequences and columns represent GO terms. The matrix was converted to a GAF file to be used for subsequent evaluations. This dataset is referred to as the "maize-FANN-GO annotations."

\paragraph{PANNZER\\}
Maize protein sequences were pre-processed using BLASTP to query a local UniProt protein BLAST database, and the output was saved in XML format as required by PANNZER. PANNZER was run on the XMl file output from the previous step, and the output was converted into a GAF file. This dataset from PANNZER is referred to as the "maize-PANNZER GO annotations."

\subsection{Cleaning and Combining Component Datasets}

\subsubsection{Score Threshold Selection for Mixed-Methods} \label{subsubsec:filter_pipeline}
Mixed-method pipelines used in maize-GAMER project provide a confidence score for each GO annotation. The confidence score ranges from 0.0-1.0, where a higher score indicates more confidence for a given annotation. A score threshold which maximizes $hF_1$ score ($hF_{max}$) will select the optimal set of annotations which reduces the total number of false-positives and false-negatives. The range of annotation scores from the mixed-method pipelines did not span the whole 0.0-1.0 range, so the scores were normalized between 0.0-1.0 independently for each annotation set. Following similar steps used to calculate $F_{max}$ in CAFA1 (CAFA1 used $Pr$ and $Rc$ metrics which are different from $hPr$ and $hRc$), $hF_{max}$ was calculated using $hPr$ and $hRc$ for mixed-method pipelines used in maize-GAMER. The $hF_{max}$ was calculated independently for each GO category for each mixed-method pipeline (See Supplementary Table \ref{tbl:pipe_score_filt}). The score thresholds which resulted in $hF_{max}$ were used to select a subset of annotations from each mixed-method pipeline. The maize-Argot2, maize-FANN-GO, maize-PANNZER GO annotations mentioned in the text following this section will refer to the subset of annotations selected in this step.

\subsubsection{Removing Redundancy and Duplication}
Duplication is the presence of two or more instances of the same gene-GO term pair in a single annotation set. Redundancy is the presence of a broader GO term in the annotations of a gene which also contains a specific annotations from which the broader GO term can be inferred by propagation. Component annotation sets from all methods described above were cleaned by removing redundancy and duplication for each annotation set across all three GO categories (See Table \ref{tbl:metrics}). Duplication was cleaned by replacing multiple instances of a gene-go term pair with a single instance for a given annotation set. Duplicate annotations from all 6 raw annotation sets were removed and files with non-duplicate annotations were created for each annotation set. Redundancy was cleaned by removing annotations containing GO terms that could be inferred from other terms based on the GO hierarchy, and only retaining the annotations with GO terms that cannot be inferred.

\subsubsection{The maize-GAMER Aggregate Dataset}
Clean (non-redundant and non-duplicated) annotation sets from all component methods were merged to generate the maize-GAMER Aggregate annotation set. Redundancy and duplication introduced by concatenating multiple datasets were removed.

A new genome assembly (B73 RefGen\_v4) and annotation set (Zm00001.2) for maize inbred line B73 was recently released (\cite{jiao_2017-Dk}). Because this dataset has not been available for long, only few published analyses are available and the research community is only now in the process of transitioning to general use of RefGen\_v4 for large-scale analyses. As such, analyses and results described here derive from the well-annotated v3 assembly and annotation set. To extend outcomes of the work described here for future v4 efforts, maize-GAMER Aggregate annotations have also been created for the maize B73 RefGen\_v4, which can be accessed at MaizeGDB (\href{http://download.maizegdb.org/maize-GAMER}{http://download.maizegdb.org/maize-GAMER}) and via CyVerse (\href{http://doi.org/10.7946/P2M925}{doi.org/10.7946/P2M925}).

\subsection{Evaluation of GAMER-derived Annotation Sets} \label{subsec:game_eval}
Component and Aggregate annotation sets were compared at two levels; a general comparison, and a GO category-specific comparison.

%general comparison of datasets
Three different metrics were calculated for the general comparison: coverage, number of annotations, and specificity (See Table \ref{tbl:metrics}). All metrics were calculated independently for each annotation set, and compared among component annotation sets as well as the aggregate annotation set. Coverage and the number of annotations were calculated directly for each annotation set. Specificity was calculated for each annotation and the mean across all annotations is reported.

%GO category specific evaluations
The annotations from component annotation sets and the aggregate annotation set were divided into specific GO categories and category-specific annotations were evaluated separately. Three different metrics were used for GO category-specific evaluations: coverage, number of annotations, and $hF_1$.  Coverage and the number of annotations were calculated individually for each GO category for each dataset. $hPr$ and $hRc$ were calculated as mentioned in \cite{defoinplatel_2011-rs}. $hF_1$ scores were calculated individually for each gene using $hPr$ and $hRc$ and then averaging the $hF_1$ scores across all genes.

\subsection{Comparisons among the maize-GAMER Aggregate, Gramene, and Phytozome Annotation Sets}
The existing Gramene, Phytozome, and maize-GAMER annotations were compared to each other. Redundancy and duplication were removed from the Gramene and Phytozome annotation sets before evaluations were performed. Evaluation and comparisons were identical to the analyses performed in the previous section \ref{subsec:game_eval}.
General evaluations for the maize-GAMER, Gramene, and Phytozome annotation sets were based on coverage, number of annotations, and specificity. These metrics were calculated as described in the previous section \ref{subsec:game_eval}.

The Gramene, Phytozome, and maize-GAMER annotation sets were also compared in a GO category-specific manner to account for biases in performance among different categories (i.e., CC, BP, and MF). Comparisons were made based on coverage, number of annotations, and mean $F_1$ score.

\subsection{Case Study of the Gene \emph{nana plant1} (\emph{na1})}
The gene \emph{na1} (GRMZM2G449033) had the most terms (7 GO terms) associated with it in the gold standard dataset, and all the terms were from BP GO category. Annotations for \emph{na1} from the three maize annotation sets were obtained. The ancestral nodes were inferred from the leaf nodes for each annotation set, and a subgraph for biological process ontology was generated (See Fig \ref{fig:na1_diagram}). The nodes in the subgraph were compared to gold standard and nodes shared between a given annotation set and the gold standard dataset were identified. Nodes exclusively found only in a given annotation set or the gold standard were also identified. Illustrations of the subgraphs without node labels were drawn to compare between the three different GO annotation sets.