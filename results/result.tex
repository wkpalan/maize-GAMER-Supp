\section{Results}
\subsection{Evaluation of maize-GAMER derived Annotation Sets}
maize-GAMER derived component annotation sets (i.e., the TAIR, UniProt, IPRS, Argot2, FANN-GO, and PANNZER) and the maize-GAMER aggregate annotation set were evaluated across GO categories as well as within each GO category using metrics described in Table \ref{tbl:metrics}.

\subsubsection{General evaluation of maize-GAMER Derived Annotation Sets}
Initial evaluations and comparisons were assessed based on coverage, number of annotations, and specificity among all clean component annotation sets as well as the aggregate annotation set (See Table \ref{tbl:metrics} \& \ref{tbl:gam-overall}). The TAIR and UniProt annotation sets had the lowest coverage and number of annotations among all maize-GAMER-derived annotation sets (Table \ref{tbl:gam-overall}). The Argot2, FANN-GO, and PANNZER annotation sets had the highest number of annotations compared to other annotation sets, as well as higher coverage compared to other annotation sets. Notably, FANN-GO had the highest coverage at 100\% of genes, and Argot2 had annotations for more than 90\% of the genes. The IPRS annotation set had a lower number of annotations compared to the CAFA mixed-method pipelines, but covered more genes than sequence-similarity methods. Although sequence-similarity methods and IPRS covered a lower number of genes, they had higher specificity compared to mixed-method pipelines in general. Of the three mixed-method pipelines, only PANNZER had comparable specificity to the sequence-similarity methods, but had lower coverage than both Argot2 and FANN-GO. Both Argot2 and FANN-GO had lower average specificity, but had higher coverage than other methods.
%This indicates both Argot2 and FANN-GO annotate more high-level (rootward) GO terms than the other methods.

The maize-GAMER aggregate annotation set covered all maize genes with at least one annotation (as expected given that the FANN-GO component annotation set also covers 100\% of genes). In addition, the aggregate annotation set contains more than double the number of annotations that occur in any component annotation set. This indicates that different component methods assign different GO terms to genes. Therefore, combining annotations from different methods results in increased diversity of GO term assignments. Moreover, the aggregate annotation set has higher specificity than the mixed-method pipelines which have higher coverage, but has lower specificity than all other component annotation sets.

Genes that are annotated with at least one GO term from each component annotation set were compared among the three different method types (i.e., sequence-similarity, domain-based, and mixed-methods; see Fig \ref{fig:maize_gam_eval}a). This comparison revealed that less than a quarter of genes had been annotated by all three methods, but more than half were annotated by two different methods. The remainder were only annotated by mixed-method pipelines. Sequence-similarity and domain-based methods resulted in annotations to genes that were also annotated by mixed-method pipelines. The number of genes annotated by domain-based methods and mixed-method pipelines, and are not annotated by sequence similarity based methods are higher than genes annotated by all three methods. In contrast, sequence-similarity methods shared more genes with both other annotation sets than only with mixed-method pipelines. Moreover, only mixed-method pipelines annotate at least one GO term to all genes in the maize FGS.

Although the mixed-method pipelines annotated all genes, they did not capture all GO terms annotated to genes by the other methods. GO term assignments were compared to evaluate the diversity of the GO terms present in the three types of GO annotation methods (See Fig \ref{fig:maize_gam_eval}b). GO terms annotated directly and ancestral terms inferred from the direct terms annotated to genes were compared among the three GO annotation methods used in maize-GAMER. The number of GO terms annotated by sequence-similarity, domain-based, and mixed-method pipelines were \num{3794}, \num{8145}, and \num{14225}, respectively. The number of GO terms annotated by the mixed-method pipelines are significantly higher than both other methods, however there are a small number of GO terms that are only annotated by sequence-similarity (721) and domain-presence (16) methods. Only a small proportion (23.05\%) of the total (\num{15028}) GO terms are annotated by all three methods.

\subsubsection{GO Category-specific Evaluations of maize-GAMER Derived Annotation Sets}
CAFA1 indicated that annotations for some GO categories are easier to predict than others (\cite{radivojac_2013-YN}). This indicated that the GO category specific evaluations could provide a more accurate comparison between component methods. This would also allow unbiased comparison of tools which do not predict certain categories (e.g. FANN-GO doesn't predict the CC category). Therefore, maize-GAMER derived annotation sets were divided into specific GO categories (i.e., CC, BP, and MF) and each category was evaluated separately based on coverage, number of annotations, and $hF_1$. 

Mixed-method pipelines had higher coverage across all three GO categories. Argot2 covered more than 80\% genes across all three categories (See Fig \ref{fig:maize_gam_eval}c). FANN-GO does not annotate GO terms for CC category, but had 100\% coverage in BP category, and covered about 50\% genes in MF category. PANNZER had the lowest coverage compared to the other mixed-method pipelines, and covered only 30-50\% of genes across different categories, and had highest coverage in BP. Sequence-similarity methods consistently had lowest coverage compared to other methods in BP and MF, but IPRS had the lowest coverage in CC. IPRS covered higher number of genes than sequence-similarity methods in BP and MF, but had lower coverage than mixed-method pipelines. When comparing IPRS coverage across three GO categories, the coverage was highest in MF.  Aggregate annotation set covered slightly more genes than the component annotation sets with highest coverage in each category, and covered more than 88\% of the FGS genes in all categories. In the BP category, the aggregate annotation set annotated all genes from maize FGS with at least one annotation.

Mixed-method pipelines produce higher number of annotations than other methods in all three GO categories. Moreover, the number of annotations from mixed-method pipelines loosely correlate with coverage in different GO categories. The only exception was PANNZER, which annotated more GO terms per gene in BP category (data not shown), than any other component annotation set. The number of annotations from sequence-similarity methods and IPRS were consistently lower than mixed-method pipelines. The variation in the number of annotations was proportional to the number of genes annotated in sequence-similarity and IPRS methods. The lowest number of annotations was seen in the CC category from IPRS, and sequence-similarity methods in other GO categories. As the union of all component method annotations, the aggregate annotation set had a higher number of annotations in all three GO categories. The highest number of annotations for the aggregate annotation set was from the BP GO category, followed by CC and MF.

$hF_1$ score represents the quality of annotations. As described in materials and methods, the $hF_1$ was calculated individually for all genes in the gold standard dataset and then averaged across all genes within an annotation set. There are clear differences in $hF_1$ across different GO categories. The highest performance was seen in the MF category, and the lowest performance is seen in the BP category. This fits the observation from CAFA1 (\cite{radivojac_2013-YN}). Mixed-method pipelines outperformed other methods in all three GO categories. PANNZER produced the highest $hF_1$ within the MF category, but Argot2 had the highest $hF_1$ scores in CC and BP. IPRS outperformed sequence-similarity methods in both MF and BP categories, but was the lowest performing method in the CC category. Comparison between two sequence-similarity methods indicated that maize-UniProt method performs better than the maize-TAIR method in MF and BP categories. On the other hand, maize-TAIR method performs better than maize-UniProt method in the CC category. Aggregating the component annotations from maize-GAMER  increased the performance in the CC category. In contrast, aggregating the component annotation sets did not increase the performance compared to the top performing tool in other categories.

\subsection{Evaluation of Existing Maize GO Annotation Sets and Comparison to the maize-GAMER Aggregate Annotation Set}
Two existing maize GO annotation sets, Gramene and Phytozome, were downloaded, evaluated, cleaned (i.e., redundancies and duplicates were removed), and compared with maize-GAMER aggregate annotations (will be called maize-GAMER annotation set from now on). The same metrics used for the evaluation of maize-GAMER derived annotation sets were used for the comparison among the existing maize GO annotation sets and maize-GAMER aggregate annotation set.

\subsubsection{General Evaluation of Public Maize GO Annotation Sets}
The maize-GAMER aggregate annotations covered all genes in the maize FGS with at least one GO term, but Gramene and Phytozome covered only about half the genes (See Table \ref{tbl:exist-overall}). Phytozome covered the fewest genes (less than half of the genes), and Gramene covered slightly more than half of the genes (See Table \ref{tbl:exist-overall}). The maize-GAMER annotation set had more annotations than both Gramene and Phytozome. Gramene had two-fold more annotations than Phytozome, and the maize-GAMER had several fold more annotations than Gramene. While the maize-GAMER has higher coverage and a higher number of annotations, it has lower average specificity than Gramene and Phytozome. Gramene has the highest average specificity of all three annotation sets.

Genes with annotations from each set were compared to see the distribution of annotated genes among different annotations (See Fig \ref{fig:maize_dataset_eval}a). Genes from Gramene and Phytozome annotations were a subset of the maize-GAMER annotations. Less than half of the genes were annotated in all three sets, and slightly more than half of the genes were annotated in at least two sets. Comparison of Gramene and Phytozome annotations show that most of the genes that were annotated were shared. Both Gramene and Phytozome had genes that were annotated in only one of the two (i.e., Gramene or Phytozome but not both; See Fig \ref{fig:maize_dataset_eval}a).

GO terms annotated directly to genes by different methods and ancestral GO terms propagated from these annotations were compared among the three annotation sets. The number of GO terms annotated in each set varied greatly. The least diverse set in terms of number of GO terms annotated was Phytozome, which was annotated with only \num{3234} GO terms ($\approx$7\% of total GO terms). Gramene has annotated \num{7215} GO terms ($\approx$16\%), and was more diverse than Phytozome, but had lower diversity than maize-GAMER. maize-GAMER had the highest diversity and contained \num{15028} GO terms ($\approx$33\%). A small number of GO terms were used by all three annotation sets, and the majority of terms from Phytozome were shared across all three annotation sets. Only a single GO term was exclusive to the Phytozome annotations, and small number of terms were found to be exclusive to Gramene annotations. Slightly more than 50\% of the GO terms from maize-GAMER were unique. The maize-GAMER aggregate annotations shared a higher number of GO terms with Gramene than Phytozome.

\subsubsection{GO Category-specific Evaluations of maize-GAMER and Existing Maize GO Annotation Sets}
Annotations from the three maize GO annotation sets were analyzed in a GO category-specific manner to identify differences in performance among the different categories (See Figure \ref{fig:maize_dataset_eval}c). As was true for the component annotation sets, three different metrics were used for evaluation and comparison: coverage, number of annotations, and $hF_1$ score.

Comparison of coverage across GO categories indicated, that all annotation sets had lower coverage in CC category, compared to other categories. Both Gramene and Phytozome had lower coverage in BP than MF, but maize-GAMER had higher coverage in BP than MF. Lowest coverage for all annotation sets and categories was seen in CC category for the Phytozome annotation set, and the highest coverage was seen in the maize-GAMER aggregate annotation set in the BP category. Comparison among the three maize annotation sets indicates that the maize-GAMER annotation set had the highest coverage in all three categories by a large margin. Coverage from maize-GAMER was almost twofold of Gramene which had the next highest coverage in all GO categories. One the other hand Gramene had higher coverage than Phytozome in all three categories.

When the number of annotations were compared across different GO categories the lowest number of annotations for Gramene and Phytozome annotation sets were seen in CC category. In contrast, maize-GAMER had the lowest number of annotations in the MF category. Moreover, both Gramene and Phytozome both had a higher number of annotations in the MF category whereas maize-GAMER had the highest number of annotations in the BP category. Comparison among the annotations sets illustrated that the maize-GAMER annotation set has the highest number of annotations in all three categories. Phytozome had the fewest annotations in all three GO categories. Number of annotations loosely correlated with coverage in different GO categories for both Gramene and Phytozome. Furthermore, maize-GAMER had the highest number of annotations in the BP category. The number of annotations from the maize-GAMER annotations for the BP was several fold higher than other annotation sets ($\approx$9x that of Gramene and $\approx$28x that of Phytozome).

$hF_1$ score reflects the overall quality of the annotations produced by different pipelines used by the three maize annotation projects. Comparing performance of different pipelines across the three GO categories revealed a similar trend that was seen in the previous section. All pipelines had higher $hF_1$ scores in the MF category, and had lower $hF_1$ scores in the BP category. The only pipeline that did not fit this trend was Phytozome, which had lowest performance in the CC category. maize-GAMER had higher $hF_1$ score than other pipelines in the CC category. maize-GAMER also had higher performance than Phytozome in other categories, but performed slightly lower than Gramene in those categories. Gramene performed better than other pipelines in the MF and BP categories. Phytozome consistently had lower performance than other pipelines across all three GO categories. Phytozome's performance was especially low in the CC category, which was the lowest $hF_1$ score seen for any annotation set in any GO category.

\subsection{Example Annotations from \emph{nana plant1} (\emph{na1})}
\emph{nana plant1} (\emph{na1}) is the gene (GRMZM2G449033) with the most annotations in the gold standard dataset. \emph{na1} is a classical maize mutant that has a dwarf phenotype (\cite{hartwig_2011-1P}). The loss-of-function mutation in the gene affects the brassenosteroid (BR) biosynthetic pathway, and BR is a plant hormone that is required for normal plant growth (\cite{hartwig_2011-1P}.)  \emph{na1} gene had 7 BP GO terms annotated to it in the gold standard dataset. Annotations for \emph{na1} from different maize annotation sets were compared to the gold standard, and a subgraph for each annotation set and gold standard dataset was plotted (See Fig \ref{fig:na1_diagram}). Phytozome did not annotate any GO terms to \emph{na1} (See Fig \ref{fig:phytozome_na1}), but both Gramene (See Fig \ref{fig:gramene_na1}) and maize-GAMER (See Fig \ref{fig:gamer_na1}) have annotated BP GO terms for \emph{na1}. Gramene annotates 3 GO terms \emph{na1} while GAMER has annotated 13 GO terms to \emph{na1}. Two GO terms from the gold standard are known to be related to \emph{na1} dwarf phenotype from previous studies, "brassinosteroid biosynthetic process" (GO:0016132) and "unidimensional cell growth" (GO:0009826). While both of these were annotated correctly by maize-GAMER (See Fig \ref{fig:gamer_na1}), only one of them was correctly annotated by Gramene (See Fig \ref{fig:gramene_na1}). Comparison of overlapping nodes indicates that the maize-GAMER aggregate annotation set also contains a number of less specific non-leaf terms which overlap with nodes inferred from gold standard dataset. Overall, the maize-GAMER has larger proportion of overlapping nodes with the gold standard than the Gramene for the BP GO category.

The different approaches taken by the pipelines from Gramene and maize-GAMER result in different annotations for the example case study of the maize \emph{na1} gene. Gramene has a lower number of GO terms annotated to \emph{na1} than maize-GAMER. The average specificity of GO terms annotated in the BP category for \emph{na1} (See Figure \ref{fig:na1_diagram}) is not significantly different between GAMER (mean=12.154) and Gramene (mean=12.667) pipelines (2-sided 2-group Wilcoxon rank-sum test; p = 0.89). This example from \emph{na1} indicates that the specificity of the annotations are not significantly different for specific instances, but are different when compared overall.