%Files from TAIR (v10) containing the protein sequences (FASTA) and gene ontology annotations (gaf) were downloaded and used for annotation. TAIR GO dataset contained a total of 261,255 annotations for 28,350 Arabidopsis genes. Only 180,774 (~ 69\%) <TILDE NOT SHOWING UP> annotations that represent 26,058 (98\%) genes were high-confidence as defined in the introduction.
%Arabidopsis protein sequences from TAIR were matched to maize protein sequences, and putative orthologs were assigned using the Reciprocal-best-hit (RBH) method. In total, 9,821 putative ortholog pairs were retained after RBH filtering. GO annotations were inherited from the Arabidopsis gene to the maize gene in each putative ortholog pair. Only high-confidence annotations from Arabidopsis were inherited to maize genes. This allows us to select the annotations from Arabidopsis which are supported by both curation and biological data. This dataset was labelled maize-TAIR dataset and was used as such in subsequent analyses. <Large sections of this text seem redundant from the methods section – I recommend removing it and focus on the results - CMA>

%UniProt plant dataset was obtained by downloading all the plant protein sequences that had at least one high-confidence GO annotation from Uniprot GOA database. Protein sequences and high-confidence GO annotations were downloaded separately in different formats. The UniProt GO annotation dataset contained 304,426 annotations from 75,537 unique protein sequences. The protein sequences downloaded spanned across 292 different taxons. When taxon were ranked by the number of annotations in the dataset, the top five species are A.thaliana (thale), Glycine max (soybean), Oryza sativa japonica (rice), Populus trichocarpa (cottonwood), and Solanum lycopersicum (tomato) (see supplementary table XXX). Only 10 species had more than 1000 annotations, and only 2,836 (0.9403\%) annotations were represented by the rest of the 282 taxons. The annotations were filtered to only include the top 10 species ranked by number of annotations. Maize proteins were matched against protein sequences from each species separately and putative orthologs were determined using RBH for each maize-plant pair. HC GO annotations from the other plant protein was inherited to the maize gene for each putative ortholog pair. All the GO annotations were concatenated together. The resulting dataset contained 106,053 annotations for 8,023 maize genes. The dataset was named maize-UniProt-Plant dataset and was used for evaluations later in the pipeline.

%Domain Presence based GO Annotations
%InterProScan (IPRS) pipeline was used to detect presence of protein domains and annotated GO terms to maize genes based on those domains. IPRS was chosen, because the tools and databases included are the top performing tools in the field of protein domain characterization (REF). The IPRS pipeline runs each input sequence through 13 different analyses (e.g. Pfam, PANTHER, SUPERFAMILY etc), and assigns one or more domains if the score from the analysis passes the default threshold. So, a gene could be assigned multiple domains from one or more analyses up to 13 different analyses. ** IPRS pipeline produced a total of 273,902 domain assignments for maize genes, but only 100,676 of these assignments annotated GO terms to genes. Out of total 39,469 maize genes, only 31,982 had domain assignments, and only 20,987 out of those have at least one GO term annotation. ** <I had a hard time following the last couple sentences- CMA> In total, IPRS pipeline produced 200,324 GO annotations for 20,987 maize genes. The dataset was renamed maize-IPRS dataset and was used in evaluations later in the pipeline.

%subsubsection{%Mixed-method pipeline based GO Annotations}
%Three different mixed-method pipelines from CAFA competition were used annotate maize protein sequences, namely Argot2, FANN-GO and PANNZER. FANN-GO and PANNZER could be downloaded and run locally, and Argot2’s batch mode allowed annotations of up to 5000 sequences at a time. CAFA competition recommends each tool/pipeline to calculate a score for each annotation to indicate the confidence of the given annotation. Scores calculated by the tools are usually between 0 and 1 or the scores are normalized to this range. Higher score indicates greater confidence for the given annotation. All mixed-method pipelines generated higher number of total annotations compared to previous methods, this increase is due to the inclusion of lower confidence annotations.

%FANN-GO produced 84,147,909 annotations representing 39,469 genes. Argot2 assigned 5,934,727 annotations to 36,570 genes. PANNZER annotated 19,477 genes with 2,134,286 annotations. The raw scores from mixed-method pipelines annotations were normalized between 0.0 - 1.0. As mentioned before, the large number of annotations produced by these pipelines are due to the retention of lower confidence annotations.

%The best subset of the predicted annotations from mixed-method pipelines were selected by evaluating the annotations using the gold standard GO annotations from MaizeGDB. Maximum avgerage F-score (max F-score) was calculated for the annotations from each tool for each ontology (See methods for details). Max F-score represents the best subset of annotations that can be selected from all the annotations from each dataset from mixed-method pipelines. Only a small percentage of annotations were retained after selecting the best subset of annotations for mixed-method pipelines (Argot2: ~6.74\%, FANN-GO: ~0.74\% and PANNZER: ~51\%). The best subset of annotations selected from each tool were named maize-Argot2 dataset, maize-FANN-GO dataset, and maize-PANNZER dataset.


%<DUPLICATE SECTIONS - my edits are in the methods - CMA>
%\subsection{%Cleaning \& Combining GO Annotation Method datasets}

%\subsubsection{%Removing Redundancy and Duplication}
%Closer inspection of the annotation datasets from different GO annotation methods revealed two different factors that were influencing the variation of GO annotations, namely duplication and redundancy. Duplication is defined as the same gene annotated with the same GO term multiple times in a single annotation dataset. This clearly inflates the number of annotations, and could potentially bias evaluations in GO annotation datasets, especially in computational annotation datasets.

%The GO terms are hierarchically related to other terms on a directed acyclic graph. This enables ancestral terms to be inherited from descendant terms. Redundancy is when a gene is annotated to a descendent term and is also annotated to one or more ancestral terms  of the specific descendant term. Redundancy also inflates the number of annotations in the dataset and could potentially bias the following evaluations.

%Duplication and Redundancy are also seen in manually curated datasets. Duplication indicates different types of evidences supporting the same annotation, and redundancy indicates evidences that support different levels of GO hierarchy. This is not the case in high-throughput computational annotations, where all annotations will get the same evidence code (IEA). GO annotation dataset created by each method was cleaned by removing redundant and duplicate annotations. While all methods had at least some redundancy or duplication, the numbers varied from method to method (See table XXX). Higher duplication was seen in maize-UniProt-Plant dataset from sequence similarity method and maize-IPRS dataset from domain presence based method. Mixed-method pipelines had the highest redundancy than other methods. Mixed-method pipelines used in maize-GAMER have been designed to assign a score to all GO terms, and do not have steps to reduce redundancy when the results are reported.
%All datasets were cleaned by removing duplication and redundancy, and clean datasets were created for all 6 datasets from maize-GAMER. The number of annotations retained after cleaning was substantially lower for most datasets (see Table XXX). With the combined cleaning of both redundancy and duplication most datasets only retained less than half of the raw annotations. Only dataset that retained more than 50\% original after cleaning was the maize-TAIR dataset. All other datasets were either highly redundant or highly duplicated.

%\subsubsection{%maize-GAMER Aggregate Dataset}

%<Should we call these annotations rather than subsets for clarity - CMA>
%The clean datasets from GO annotation methods were combined together to create an aggregate dataset for maize-GAMER. The aggregate dataset is the union set of GO annotation datasets from different tools (i.e. Each annotation in the aggregate dataset was present in one or more GO annotation method datasets). Some annotations in the aggregate dataset were supported by multiple GO annotation method datasets, and this introduced duplication to the aggregate dataset. Similarly, some GO annotation methods assigned GO terms at different levels of GO hierarchy, and this introduced redundancy. The aggregate dataset was cleaned by removing duplication and redundancy.