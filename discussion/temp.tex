The datasets produced by the GAMER pipeline and were evaluated a 
Evaluation of the quality of the datasets produced by different GO annotation methods against the gold standard dataset was 
The pipeline combines the production of the annotation methods and the evaluation of the datasets in the same process to produce and evaluate the  
The maize-GAMER aggregate dataset has more coverage compared to both Gramene and Phytozome datasets, and has also significantly increased the number of annotations for the maize genes. Component datasets from mixed-method pipelines have covered all maize FGS genes, thus increased coverage is attributed to the mixed-method pipelines. Number of annotations are influenced by two parameters, namely the number of genes covered and number of GO terms used to annotate them. Mixed-method pipelines annotated more diverse GO terms compared to both sequence-similarity and domain-presence based methods, and this resulted in increased number of annotations. Lower number of annotations from Phytozome was the result of lower coverage and low number of GO terms annotated in the dataset. Gramene and Phytozome datasets have slightly higher average specificity than the maize-GAMER dataset.

The quality of the GO annotation methods producing the maize GO datasets were assessed using two metrics. Performance was evaluated by comparing the predicted datasets to the gold standard and calculating the average hierarchical F-score (hF-score), and informativeness of the annotations were evaluated by average specificity (See Table \ref{tbl:metrics}). The maize-GAMER aggregate dataset has comparable or better performance than Gramene depending on the GO category. Moreover, maize-GAMER dataset performed better than Phytozome in all GO categories. Quality was assessed for each GO category separately because of the difficulty in predicting the BP category (\cite{radivojac_2013-YN}). Mean hF-score is influenced by several factors, namely coverage, number of annotations and specificity. Increase in coverage will positively affect the F-score, but only if the correct GO terms annotated to the newly annotated genes. The hF-Score will decrease with the number of annotations, because the gold standard dataset is sparsely annotated. Only a small subset of the predicted annotations for each gene can be assessed effectively because the gold standard dataset is sparse. Regardless whether the other annotations are correct or not, they will be considered false positives. Specificity of the annotations is generally positively correlated to the hF-score, because annotations in the gold standard tend to have higher specificity. 

The dataset with a higher average specificity annotates more GO terms further from the root, and further from the root the GO term is more informative it is. Both Gramene and Phytozome datasets have higher average specificity than the maize-GAMER dataset. Lower specificity of the maize-GAMER dataset is due to the inclusion of broader GO terms annotated by the mixed-method pipelines. The Influence on average specificity by the annotations from mixed-method pipelines are likely mitigated by the inclusion of annotations from other methods with higher specificity and removing redundancy. The mitigation is evident, because the aggregate dataset has higher specificity than the mixed-method pipelines with the lowest specificity. 

Overall differences in design of the pipelines can be seen from the example annotations from \textit{na1} (See Fig \ref{fig:na1_diagram}). Lack of annotations from Phytozome is representative of the low coverage and low number of annotations in the dataset. Gramene annotates lower number of GO terms with higher specificity to the genes, where as GAMER annotates more GO terms to genes and only some of them have higher specificity. 



\begin{itemize}
  \item Have implemented a reproducible pipeline - something about what data democratization is and about what democratization of annotation means (AKA we are no longer held hostage by whoever is the genome sequencing project.  No one is ‘anointed’ to do this work)
  \begin{itemize}
    \item Neither Gramene nor Phytozome annotations are reproducible by others in an apparent manner
    \item Reproducibility is important for doing science and to enable comparison among datasets and methods that generate them
  \end{itemize}
  \item Increased the number of annotated genes and annotations
  \begin{itemize}
    \item Why number of annotations increased
    \item Number of annotations are a function of Genes annotated and GO terms used to annotate them.
    \item Higher coverage, and high number of GO terms used by MM pipes increased \# of genes
    \item Lower coverage and GO terms reason for lower annotations in Phytozome
  \end{itemize}
  \item Increased or comparable quality of the annotations
  \begin{itemize}
    \item Fig \& Tbl 4: Reasons for Avg F-score differences between datasets for different GO categories
    \begin{itemize}
      \item Coverage - increased coverage improves score (less false negatives)
      \item Specificity - increased specificity increases score because gold standard has higher specificity (less false negatives)
    \end{itemize}
    \item Fig 5:
    \begin{itemize}
      \item Higher specificity in Gramene but only few annotations
      \item Lower specificity but higher number of annotations in maize-GAMER dataset
      \item 5 leaf terms annotated correctly in MG vs 1 in Gram
      \item Issues of evaluation with lower specificity GS annotations
    \end{itemize}
  \end{itemize}
  \item Dissection of the pipeline and some individual modules
  \begin{itemize}
    \item Strengths and Weaknesses of different GAMs
    \item Strengths and Weaknesses of the pipeline and the dataset
  \end{itemize}
  \item Something about how CAFA enables improved annotation across species - CAFA is USEFUL, not just fun :hy)
  \item Quip about AgriGO and something about how availability of such datasets would enable AgriGO and other such tools could populate with various datasets (?)
    \begin{itemize}
      \item The dataset currently in AgriGO is older version of Gramene (AGPv1), and how this dataset could improve the situation
      \item Make a tool which allows users to select the dataset that they want to use
    \end{itemize}
\end{itemize}

, which is a community challenge for assessing protein function prediction methods. CAFA competition has brought diverse researchers working on the protein function prediction problem together, and provided an set of unbiased methods to assess GO annotations predicted by competing tools.