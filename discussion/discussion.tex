\section{Discussion}
Our main goals for the maize-GAMER project were to improve the GO annotation dataset for maize and to document inputs, methods, and results at a level to enable both reproducibility and reuse of the pipeline for future genome versions. 

To determine how best to create an improved maize annotation dataset, we tried out multiple different methods and compared the resulting datasets to a gold-standard to better understand differences in term assignments among the methods we used.  Using the same gold-standard, we also were able to compare our resulting datasets to those produced by and available from Gramene and Phytozome. 

We used the unbiased assessment methods developed for CAFA and adapted a strategy from CAFA ($F_{max}$) to set thresholds for the mixed-method pipelines we used as well as for comparing among datasets resulting from all methods under evaluation. We found that mixed-method pipelines developed for the CAFA1 challenge outperformed RBH and domain-based methods for GO annotation (\cite{radivojac_2013-YN}). They covered more genes with annotations, produced higher number of annotations, and had higher F-score than both sequence-similarity and domain based methods. The higher performance from mixed-method pipelines are the outcome of advanced statistical (\cite{falda_2012-VX} and \cite{koskinen_2015-sl}) and machine learning approaches (\cite{clark_2011--Z}) used to reduce the false positive and false negative annotations. Mixed-method pipelines do have a limitation: they have higher coverage but annotations are less specific in general when compared with datasets produced using other approaches. This could be due to the dearth of training dataset for the more specific GO terms, which is required for training machine learning methods. 

When we aggregated the predictions from RBH, domain-based methods, and three tools from CAFA1, we produced the maize-GAMER aggregate dataset, which covers more gene space than the datasets produced by Gramene and Phytozome, and with similar or better accuracy. Next, we turned the methods we used to create the maize-GAMER dataset into a pipeline we call GO-MAP and deposited our input and resulting datasets in public repositories.  The GO-MAP pipeline can be used for GO annotation of newly sequenced plant genomes as well as existing plant genomes. The pipeline is freely available, and the exact steps and parameters which were used for maize have been supplied as a supplementary document to facilitate reproducibility. 

The set of manually reviewed gene function annotations for maize that we call the gold-standard is both incomplete and sparse. This situation does not reflect the amount of published literature describing gene function for maize. Instead, this situation is due to limited curation of gene function into GO terms. While tools exist at MaizeGDB that enable researchers to assign GO terms to genes, these tools remain poorly utilized.  In an effort to improve community engagement and to upgrade the evidence codes for GO assignments, our next step for GAMER will be to develop and deploy a tool to enable experts in the maize community to review existing GO annotations. By enabling GO annotation review through expert crowdsourcing, term assignments produced by computational pipelines including GAMER can be upgraded from IEA (inferred from electronic annotation) to RCA (reviewed computational analysis). In this way, we hope that way to support the transfer the collective knowledge maize community has generated over the years to produce higher-quality functional annotation datasets for maize and other species. 

%Our main goal for the maize-GAMER project was to improve the GO annotation dataset for maize. This was accomplished by designing GO-MAP, a reproducible meta annotator for large-scale GO term annotation of plant genes. GO-MAP was used to generate maize annotations, then we evaluated GO annotations produced for maize as well as other public maize GO datasets.

%GO-MAP uses diverse GO annotation tools to predict GO annotations and aggregates the annotations to produce an improved GO annotation dataset. The GO-MAP pipeline can be used for GO annotation of newly sequenced plant genomes as well as existing plant genomes. The pipeline is freely available, and the exact steps and parameters which were used for maize have been supplied as a supplementary document to facilitate reproducibility. Large scale pipelines used by Gramene and Phytozome do not have the necessary information (e.g. tools, versions, and parameters) to reproduce the annotations produced for plants. Moreover, the maize-GAMER project also assessed the annotations from the public maize annotation datasets produced by Gramene and Phytozome. Quality of public datasets have not been assessed against a gold standard dataset till now. Assessment of quality and quantity of the public maize datasets allows the researchers to make an informed choice when selecting a maize GO annotation dataset for analysis. %Availability of a large-scale reproducible pipeline allows for the democratization of the annotation dataset by allowing researchers to modify the pipeline to get annotations tailored for their own needs. %GO-MAP pipeline is tailored specifically for plants, but majority of the functional prediction tools are species agnostic %(\cite{jiang_2016-be}). While a specific pipeline improves the performance in plants, it will cannot be generalized for all species. On the other hand, adapting GO-MAP pipeline for another specific taxonomic group (e.g. insects) would take less effort.

%The mixed-method pipelines identified in CAFA1 were crucial components of the GO-MAP pipeline (\cite{radivojac_2013-YN}). Mixed-method pipelines outperformed other methods for GO annotation. They covered more genes with annotations, produced higher number of annotations, and had higher F-score than both sequence-similarity and domain based methods. The limitation of the mixed-method pipelines that have higher coverage is that the annotations are less specific in general than other approaches. This could be due to the dearth of training dataset for the more specific GO terms, which is required for training machine learning methods. The higher performance from mixed-method pipelines are the outcome of advanced statistical (\cite{falda_2012-VX} and \cite{koskinen_2015-sl}) and machine learning approaches (\cite{clark_2011--Z}) used to reduce the false positive and false negative annotations. Moreover, the unbiased assessment methods developed for CAFA were also essential for the maize-GAMER project. maize-GAMER adapted a strategy from CAFA (F\textsubscript{max}) to select annotations with higher confidence from mixed-method pipelines. Maize-GAMER project is an example of how CAFA competition has helped improve the functional annotation of species which are not routinely included in the competition.

%Evaluation and comparison of the Gramene, Phytozome and maize-GAMER datasets show that Phytozome lags behind in both quality and quantity in all GO categories. This difference could be due to the fact Phytozome uses a domain based method (Pfam2GO) for annotating GO terms. In comparison, the maize-GAMER and Gramene datasets are derived from complex mixed-method pipelines. Gramene's pipeline annotates fewer GO terms with higher specificity compared to the GO-MAP pipeline. The differences in the approaches taken by these three projects are clearly seen when the annotations for \emph{na1} are compared. The majority of genes have a higher number of GO terms annotated in the maize-GAMER dataset than the Gramene and Phytozome datasets, but there are a few genes where the Gramene dataset has more annotations than maize-GAMER. These observations indicate that while the GO-MAP pipeline used to produce the maize-GAMER dataset is an improvement, there is still room for improvement. 

%Evaluation methods rely on the quality of the gold standard dataset. The number of GO terms annotated to a gene in the gold standard dataset affects the datasets differently. In general Gramene's approach to annotate fewer GO terms results in higher F-score for genes with low number of gold standard GO annotations. On the contrary, GAMER's approach to annotate higher number of annotations results in higher F-score for genes with higher number of gold standard GO annotations. The evaluation methods applied themselves were unbiased for any specific dataset.

%Current maize gold standard dataset is incomplete and sparse. The low number of annotations in the gold standard dataset does not reflect the amount of published literature that is available for curation. The low number of annotations in the gold standard is due to limited resources available for curation. Community of researchers working on maize includes experts who focus on identifying genes regulating various biological processes. A tool to enable the experts in maize community to review existing GO annotations for maize genes will speed up the curation process. maize-GAMER project will design and implement a tool that enables expert crowdsourcing of the review of existing maize GO annotations. Implementation of such a tool is a faster way to transfer the collective knowledge maize community has generated over the years to functional annotation datasets. 

%GO term over-representation analysis is usually performed based on a single GO annotation dataset. Currently, maize has three 3 public GO annotation datasets (mentioned in previous sections), and it could have more datasets which are not mentioned here. Each dataset has it's own strengths and weaknesses. Running over-representation analysis independently on each dataset would allow researchers to compare the results from different datasets. This comparison will be invaluable in making informed choices of experiments for validation. maize-GAMER project will design a simple web tool, which allows for researchers to select a specific GO annotation dataset available for maize and run over-representation analysis. The tool will be implemented based on existing R packages and will allow the users to perform enrichment analysis without the need for coding. Alleviating the need for coding allows the datasets and over-representation analysis to be accessible to the larger maize community.

%Methods used for GO annotations produce varying results.
%There could be little to no overlap between predicted annotations from different tools.
%Aggregation of results from multiple tools could provide a more comprehensive set of GO annotations for maize.

%Deeper understanding and evaluation of GO annotations are essential to produce a best subset of GO annotations that reflect the biology.

%GO annotations assigned by the pipeline will be given an Inferred from Electronic Annotation (IEA) evidence code. While this meets the required guidelines of GO consortium, it does not differentiate the type of analysis that was used to assign that GO term. We will adapt an existing ranking system such as Evidence Ontology (ECO), or design a 5 star rating system with explicit rules to assign confidence to the GO annotations.

%There is no better alternative than the maize community itself to further improve the functional annotation of maize. Any small effort by the community will significantly improve the confidence of maize functional annotations. We are now designing a user-friendly and easy-to-use review system in collaboration with MaizeGDB to enable community review of GO annotations.
